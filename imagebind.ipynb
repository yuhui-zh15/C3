{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sailhome/yuhuiz/develop/miniconda3/envs/modeldiff/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/sailhome/yuhuiz/develop/miniconda3/envs/modeldiff/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/sailhome/yuhuiz/develop/miniconda3/envs/modeldiff/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading imagebind weights to .checkpoints/imagebind_huge.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.47G/4.47G [00:28<00:00, 171MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision x Text:  tensor([[9.9761e-01, 2.3694e-03, 1.8612e-05],\n",
      "        [3.3836e-05, 9.9994e-01, 2.4119e-05],\n",
      "        [4.7996e-05, 1.3496e-02, 9.8646e-01]], device='cuda:0')\n",
      "Audio x Text:  tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]], device='cuda:0')\n",
      "Vision x Audio:  tensor([[0.8070, 0.1088, 0.0842],\n",
      "        [0.1036, 0.7884, 0.1079],\n",
      "        [0.0018, 0.0022, 0.9960]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "import torch\n",
    "from models import imagebind_model\n",
    "from models.imagebind_model import ModalityType\n",
    "\n",
    "text_list=[\"A dog.\", \"A car\", \"A bird\"]\n",
    "image_paths=[\".assets/dog_image.jpg\", \".assets/car_image.jpg\", \".assets/bird_image.jpg\"]\n",
    "audio_paths=[\".assets/dog_audio.wav\", \".assets/car_audio.wav\", \".assets/bird_audio.wav\"]\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "print(\n",
    "    \"Vision x Text: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    ")\n",
    "print(\n",
    "    \"Audio x Text: \",\n",
    "    torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1),\n",
    ")\n",
    "print(\n",
    "    \"Vision x Audio: \",\n",
    "    torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T, dim=-1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19.8767, 19.0552, 19.0818], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['audio'].norm(dim=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio-Text: Clotho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3938 2893 1045\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "import data\n",
    "import torch\n",
    "from models import imagebind_model\n",
    "from models.imagebind_model import ModalityType\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CLOTHO_PATH = \"/pasteur/u/yuhuiz/data/CLOTHO/\"\n",
    "\n",
    "# Read CSV from clotho_captions_development.csv\n",
    "clotho_df_train = pd.read_csv(CLOTHO_PATH + \"clotho_captions_development.csv\")\n",
    "file_names_train = clotho_df_train['file_name'].tolist()\n",
    "file_names_train = [CLOTHO_PATH + \"development/\" + file_name for file_name in file_names_train]\n",
    "\n",
    "# Read CSV from clotho_captions_evaluation.csv\n",
    "clotho_df_test = pd.read_csv(CLOTHO_PATH + \"clotho_captions_evaluation.csv\")\n",
    "file_names_test = clotho_df_test['file_name'].tolist()\n",
    "file_names_test = [CLOTHO_PATH + \"evaluation/\" + file_name for file_name in file_names_test]\n",
    "\n",
    "file_names = file_names_train + file_names_test\n",
    "\n",
    "print(len(file_names), len(file_names_train), len(file_names_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [04:41<00:00,  2.27s/it]\n"
     ]
    }
   ],
   "source": [
    "audio_embeddings = []\n",
    "\n",
    "batch_size = 32\n",
    "for i in trange(0, len(file_names), batch_size):\n",
    "    with torch.no_grad():\n",
    "        # Load data\n",
    "        inputs = {\n",
    "            ModalityType.AUDIO: data.load_and_transform_audio_data(file_names[i: i + batch_size], device),\n",
    "        }\n",
    "        embeddings = model(inputs)\n",
    "        audio_embeddings.append(embeddings[ModalityType.AUDIO].cpu())\n",
    "\n",
    "audio_embeddings = torch.cat(audio_embeddings, dim=0)\n",
    "torch.save([file_names, audio_embeddings], \"audio_embeddings_clotho.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 616/616 [02:15<00:00,  4.55it/s]\n"
     ]
    }
   ],
   "source": [
    "captions = clotho_df_train['caption_1'].tolist() + clotho_df_train['caption_2'].tolist() + clotho_df_train['caption_3'].tolist() + clotho_df_train['caption_4'].tolist() + clotho_df_train['caption_5'].tolist() + clotho_df_test['caption_1'].tolist() + clotho_df_test['caption_2'].tolist() + clotho_df_test['caption_3'].tolist() + clotho_df_test['caption_4'].tolist() + clotho_df_test['caption_5'].tolist()\n",
    "text_embeddings = []\n",
    "\n",
    "batch_size = 32\n",
    "for i in trange(0, len(captions), batch_size):\n",
    "    with torch.no_grad():\n",
    "        # Load data\n",
    "        inputs = {\n",
    "            ModalityType.TEXT: data.load_and_transform_text(captions[i: i + batch_size], device),\n",
    "        }\n",
    "        embeddings = model(inputs)\n",
    "        text_embeddings.append(embeddings[ModalityType.TEXT].cpu())\n",
    "\n",
    "text_embeddings = torch.cat(text_embeddings, dim=0)\n",
    "torch.save([captions, text_embeddings], \"text_embeddings_clotho.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3938 3938 19690 19690\n"
     ]
    }
   ],
   "source": [
    "print(len(file_names), len(audio_embeddings), len(captions), len(text_embeddings))\n",
    "audio_to_embeddings = {key: value for key, value in zip(file_names, audio_embeddings)}\n",
    "text_to_embeddings = {key: value for key, value in zip(captions, text_embeddings)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>caption_1</th>\n",
       "      <th>caption_2</th>\n",
       "      <th>caption_3</th>\n",
       "      <th>caption_4</th>\n",
       "      <th>caption_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Distorted AM Radio noise.wav</td>\n",
       "      <td>A muddled noise of broken channel of the TV</td>\n",
       "      <td>A television blares the rhythm of a static TV.</td>\n",
       "      <td>Loud television static dips in and out of focus</td>\n",
       "      <td>The loud buzz of static constantly changes pit...</td>\n",
       "      <td>heavy static and the beginnings of a signal on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paper_Parchment_Rustling.wav</td>\n",
       "      <td>A person is turning a map over and over.</td>\n",
       "      <td>A person is very carefully rapping a gift for ...</td>\n",
       "      <td>A person is very carefully wrapping a gift for...</td>\n",
       "      <td>He sighed as he turned the pages of the book, ...</td>\n",
       "      <td>papers are being turned, stopped, then turned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03 Whales Slowing Down.wav</td>\n",
       "      <td>Several barnyard animals mooing in a barn whil...</td>\n",
       "      <td>The vocalization of several whales, along with...</td>\n",
       "      <td>Underwater, large numbers of shrimp clicking a...</td>\n",
       "      <td>Whales sing to one another over the flowing wa...</td>\n",
       "      <td>wales sing to one another with water flowing i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rope tied to boat in port.wav</td>\n",
       "      <td>An office chair is squeaking as someone bends ...</td>\n",
       "      <td>Popping and squeaking gradually tapers off to ...</td>\n",
       "      <td>Someone is opening a creaky door slowly while ...</td>\n",
       "      <td>Squeaking and popping followed by gradual popp...</td>\n",
       "      <td>an office chair is squeaking as someone leans ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>carpenter bee.wav</td>\n",
       "      <td>A flying bee is buzzing loudly around an objec...</td>\n",
       "      <td>An annoying fly is buzzing loudly and consiste...</td>\n",
       "      <td>An insect buzzing in the foreground as birds c...</td>\n",
       "      <td>An insect trapped in a spider web struggles, b...</td>\n",
       "      <td>Outdoors, insect trapped in a spider web and t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file_name  \\\n",
       "0   Distorted AM Radio noise.wav   \n",
       "1   Paper_Parchment_Rustling.wav   \n",
       "2     03 Whales Slowing Down.wav   \n",
       "3  Rope tied to boat in port.wav   \n",
       "4              carpenter bee.wav   \n",
       "\n",
       "                                           caption_1  \\\n",
       "0        A muddled noise of broken channel of the TV   \n",
       "1           A person is turning a map over and over.   \n",
       "2  Several barnyard animals mooing in a barn whil...   \n",
       "3  An office chair is squeaking as someone bends ...   \n",
       "4  A flying bee is buzzing loudly around an objec...   \n",
       "\n",
       "                                           caption_2  \\\n",
       "0     A television blares the rhythm of a static TV.   \n",
       "1  A person is very carefully rapping a gift for ...   \n",
       "2  The vocalization of several whales, along with...   \n",
       "3  Popping and squeaking gradually tapers off to ...   \n",
       "4  An annoying fly is buzzing loudly and consiste...   \n",
       "\n",
       "                                           caption_3  \\\n",
       "0    Loud television static dips in and out of focus   \n",
       "1  A person is very carefully wrapping a gift for...   \n",
       "2  Underwater, large numbers of shrimp clicking a...   \n",
       "3  Someone is opening a creaky door slowly while ...   \n",
       "4  An insect buzzing in the foreground as birds c...   \n",
       "\n",
       "                                           caption_4  \\\n",
       "0  The loud buzz of static constantly changes pit...   \n",
       "1  He sighed as he turned the pages of the book, ...   \n",
       "2  Whales sing to one another over the flowing wa...   \n",
       "3  Squeaking and popping followed by gradual popp...   \n",
       "4  An insect trapped in a spider web struggles, b...   \n",
       "\n",
       "                                           caption_5  \n",
       "0  heavy static and the beginnings of a signal on...  \n",
       "1  papers are being turned, stopped, then turned ...  \n",
       "2  wales sing to one another with water flowing i...  \n",
       "3  an office chair is squeaking as someone leans ...  \n",
       "4  Outdoors, insect trapped in a spider web and t...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2893/2893 [00:00<00:00, 3565.06it/s]\n",
      "100%|██████████| 1045/1045 [00:00<00:00, 3717.84it/s]\n"
     ]
    }
   ],
   "source": [
    "display(clotho_df_train.head())\n",
    "\n",
    "# transform to x_embed (audio embedding), y_embed (caption embedding), y (caption)\n",
    "train_data = []\n",
    "\n",
    "for index, row in tqdm(clotho_df_train.iterrows(), total=len(clotho_df_train)):\n",
    "    file_name = CLOTHO_PATH + \"development/\" + row['file_name']\n",
    "    for i in range(5): # 5 captions\n",
    "        train_data.append({\n",
    "            \"x\": file_name,\n",
    "            \"y\": row['caption_' + str(i + 1)],\n",
    "            \"x_embed\": F.normalize(audio_to_embeddings[file_name], dim=0).numpy(),\n",
    "            \"y_embed\": F.normalize(text_to_embeddings[row['caption_' + str(i + 1)]], dim=0).numpy(),\n",
    "            \"split\": \"train\"\n",
    "        })\n",
    "\n",
    "test_data = []\n",
    "\n",
    "for index, row in tqdm(clotho_df_test.iterrows(), total=len(clotho_df_test)):\n",
    "    file_name = CLOTHO_PATH + \"evaluation/\" + row['file_name']\n",
    "    for i in range(5): # 5 captions\n",
    "        test_data.append({\n",
    "            \"x\": file_name,\n",
    "            \"y\": row['caption_' + str(i + 1)],\n",
    "            \"x_embed\": F.normalize(audio_to_embeddings[file_name], dim=0).numpy(),\n",
    "            \"y_embed\": F.normalize(text_to_embeddings[row['caption_' + str(i + 1)]], dim=0).numpy(),\n",
    "            \"split\": \"test\"\n",
    "        })\n",
    "\n",
    "data = train_data + test_data\n",
    "\n",
    "import pickle \n",
    "with open('data_audio_clotho_imagebind.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'x': '/pasteur/u/yuhuiz/data/CLOTHO/development/Distorted AM Radio noise.wav',\n",
       "  'y': 'A muddled noise of broken channel of the TV',\n",
       "  'x_embed': array([-0.0238224 , -0.05003118,  0.02089429, ..., -0.01378978,\n",
       "         -0.02703197,  0.02927944], dtype=float32),\n",
       "  'y_embed': array([-0.00901202,  0.00539664,  0.00060415, ...,  0.03331101,\n",
       "         -0.00200131,  0.01315751], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/CLOTHO/development/Distorted AM Radio noise.wav',\n",
       "  'y': 'A television blares the rhythm of a static TV.',\n",
       "  'x_embed': array([-0.0238224 , -0.05003118,  0.02089429, ..., -0.01378978,\n",
       "         -0.02703197,  0.02927944], dtype=float32),\n",
       "  'y_embed': array([-0.02141449,  0.02404512,  0.0194047 , ...,  0.02225818,\n",
       "         -0.01603848,  0.00652554], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/CLOTHO/development/Distorted AM Radio noise.wav',\n",
       "  'y': 'Loud television static dips in and out of focus',\n",
       "  'x_embed': array([-0.0238224 , -0.05003118,  0.02089429, ..., -0.01378978,\n",
       "         -0.02703197,  0.02927944], dtype=float32),\n",
       "  'y_embed': array([ 0.00431422,  0.01847505,  0.02238768, ..., -0.00209544,\n",
       "         -0.00732608,  0.01492506], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/CLOTHO/development/Distorted AM Radio noise.wav',\n",
       "  'y': 'The loud buzz of static constantly changes pitch and volume.',\n",
       "  'x_embed': array([-0.0238224 , -0.05003118,  0.02089429, ..., -0.01378978,\n",
       "         -0.02703197,  0.02927944], dtype=float32),\n",
       "  'y_embed': array([-0.03845963,  0.01123006,  0.06686026, ..., -0.00090524,\n",
       "          0.04202003, -0.04043188], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/CLOTHO/development/Distorted AM Radio noise.wav',\n",
       "  'y': 'heavy static and the beginnings of a signal on a transistor radio',\n",
       "  'x_embed': array([-0.0238224 , -0.05003118,  0.02089429, ..., -0.01378978,\n",
       "         -0.02703197,  0.02927944], dtype=float32),\n",
       "  'y_embed': array([ 2.6766430e-03, -2.1363195e-02,  2.1488965e-02, ...,\n",
       "         -3.2925806e-03,  7.8598612e-05, -8.2499877e-04], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/CLOTHO/development/Paper_Parchment_Rustling.wav',\n",
       "  'y': 'A person is turning a map over and over.',\n",
       "  'x_embed': array([ 0.01319109,  0.00790129,  0.03251145, ...,  0.01381426,\n",
       "         -0.0414697 ,  0.04825891], dtype=float32),\n",
       "  'y_embed': array([ 0.01806507,  0.02789235, -0.03213729, ...,  0.06222369,\n",
       "         -0.04606079,  0.00853528], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/CLOTHO/development/Paper_Parchment_Rustling.wav',\n",
       "  'y': 'A person is very carefully rapping a gift for someone else.',\n",
       "  'x_embed': array([ 0.01319109,  0.00790129,  0.03251145, ...,  0.01381426,\n",
       "         -0.0414697 ,  0.04825891], dtype=float32),\n",
       "  'y_embed': array([ 0.02777926,  0.01048102, -0.00526025, ...,  0.00452267,\n",
       "         -0.01297673,  0.02805823], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/CLOTHO/development/Paper_Parchment_Rustling.wav',\n",
       "  'y': 'A person is very carefully wrapping a gift for someone else.',\n",
       "  'x_embed': array([ 0.01319109,  0.00790129,  0.03251145, ...,  0.01381426,\n",
       "         -0.0414697 ,  0.04825891], dtype=float32),\n",
       "  'y_embed': array([ 0.07534658,  0.02399582, -0.02902931, ...,  0.05315656,\n",
       "         -0.0060981 ,  0.01206384], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/CLOTHO/development/Paper_Parchment_Rustling.wav',\n",
       "  'y': 'He sighed as he turned the pages of the book, stopping to scan the information.',\n",
       "  'x_embed': array([ 0.01319109,  0.00790129,  0.03251145, ...,  0.01381426,\n",
       "         -0.0414697 ,  0.04825891], dtype=float32),\n",
       "  'y_embed': array([-0.02057374,  0.02267773,  0.02618273, ..., -0.00181551,\n",
       "          0.02532708,  0.01300937], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/CLOTHO/development/Paper_Parchment_Rustling.wav',\n",
       "  'y': 'papers are being turned, stopped, then turned again and someone is breathing.',\n",
       "  'x_embed': array([ 0.01319109,  0.00790129,  0.03251145, ...,  0.01381426,\n",
       "         -0.0414697 ,  0.04825891], dtype=float32),\n",
       "  'y_embed': array([-0.00237856, -0.00051517,  0.04511509, ..., -0.00900938,\n",
       "         -0.01096617,  0.02216607], dtype=float32),\n",
       "  'split': 'train'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video-Text: MSR-VTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sailhome/yuhuiz/develop/miniconda3/envs/modeldiff/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/sailhome/yuhuiz/develop/miniconda3/envs/modeldiff/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 7010 2990\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "import data\n",
    "import torch\n",
    "from models import imagebind_model\n",
    "from models.imagebind_model import ModalityType\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MSRVTT_PATH = \"/pasteur/u/yuhuiz/data/MSRVTT/\"\n",
    "\n",
    "data_train = json.load(open(MSRVTT_PATH + \"train_val_videodatainfo.json\"))\n",
    "data_test = json.load(open(MSRVTT_PATH + \"test_videodatainfo.json\"))\n",
    "\n",
    "file_names_train = [MSRVTT_PATH + \"TrainValVideo/\" + data_train['videos'][i]['video_id'] + \".mp4\" for i in range(len(data_train['videos']))]\n",
    "file_names_test = [MSRVTT_PATH + \"TestVideo/\" + data_test['videos'][i]['video_id'] + \".mp4\" for i in range(len(data_test['videos']))]\n",
    "\n",
    "file_names = file_names_train + file_names_test\n",
    "\n",
    "print(len(file_names), len(file_names_train), len(file_names_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [1:46:11<00:00,  5.10s/it]\n"
     ]
    }
   ],
   "source": [
    "video_embeddings = []\n",
    "\n",
    "batch_size = 8\n",
    "for i in trange(0, len(file_names), batch_size):\n",
    "    with torch.no_grad():\n",
    "        # Load data\n",
    "        inputs = {\n",
    "            ModalityType.VISION: data.load_and_transform_video_data(file_names[i: i + batch_size], device),\n",
    "        }\n",
    "        embeddings = model(inputs)\n",
    "        video_embeddings.append(embeddings[ModalityType.VISION].cpu())\n",
    "\n",
    "video_embeddings = torch.cat(video_embeddings, dim=0)\n",
    "torch.save([file_names, video_embeddings], \"video_embeddings_msrvtt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_train = [item[\"caption\"] for item in data_train[\"sentences\"]]\n",
    "captions_test = [item[\"caption\"] for item in data_test[\"sentences\"]]\n",
    "captions = captions_train + captions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [15:33<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "text_embeddings = []\n",
    "\n",
    "batch_size = 256\n",
    "for i in trange(0, len(captions), batch_size):\n",
    "    with torch.no_grad():\n",
    "        # Load data\n",
    "        inputs = {\n",
    "            ModalityType.TEXT: data.load_and_transform_text(captions[i: i + batch_size], device),\n",
    "        }\n",
    "        embeddings = model(inputs)\n",
    "        text_embeddings.append(embeddings[ModalityType.TEXT].cpu())\n",
    "\n",
    "text_embeddings = torch.cat(text_embeddings, dim=0)\n",
    "torch.save([captions, text_embeddings], \"text_embeddings_msrvtt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000 200000 200000\n"
     ]
    }
   ],
   "source": [
    "print(len(file_names), len(video_embeddings), len(captions), len(text_embeddings))\n",
    "video_to_embeddings = {key: value for key, value in zip(file_names, video_embeddings)}\n",
    "text_to_embeddings = {key: value for key, value in zip(captions, text_embeddings)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_processed = []\n",
    "for item in data_train[\"sentences\"]:\n",
    "    caption = item[\"caption\"]\n",
    "    video_id = item[\"video_id\"]\n",
    "    video_path = MSRVTT_PATH + \"TrainValVideo/\" + video_id + \".mp4\"\n",
    "    train_data_processed.append({\n",
    "        \"x\": video_path,\n",
    "        \"y\": caption,\n",
    "        \"x_embed\": F.normalize(video_to_embeddings[video_path], dim=0).numpy(),\n",
    "        \"y_embed\": F.normalize(text_to_embeddings[caption], dim=0).numpy(),\n",
    "        \"split\": \"train\"\n",
    "    })\n",
    "\n",
    "test_data_processed = []\n",
    "for item in data_test[\"sentences\"]:\n",
    "    caption = item[\"caption\"]\n",
    "    video_id = item[\"video_id\"]\n",
    "    video_path = MSRVTT_PATH + \"TestVideo/\" + video_id + \".mp4\"\n",
    "    test_data_processed.append({\n",
    "        \"x\": video_path,\n",
    "        \"y\": caption,\n",
    "        \"x_embed\": F.normalize(video_to_embeddings[video_path], dim=0).numpy(),\n",
    "        \"y_embed\": F.normalize(text_to_embeddings[caption], dim=0).numpy(),\n",
    "        \"split\": \"test\"\n",
    "    })\n",
    "\n",
    "data = train_data_processed + test_data_processed\n",
    "\n",
    "import pickle \n",
    "with open('data_video_msrvtt_imagebind.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'x': '/pasteur/u/yuhuiz/data/MSRVTT/TrainValVideo/video2960.mp4',\n",
       "  'y': 'a cartoon animals runs through an ice cave in a video game',\n",
       "  'x_embed': array([-0.01323111,  0.01547959, -0.01875113, ..., -0.02266031,\n",
       "          0.00901074,  0.02306143], dtype=float32),\n",
       "  'y_embed': array([-0.0148942 ,  0.0271524 , -0.04431538, ..., -0.00151742,\n",
       "         -0.0303146 ,  0.03866813], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/MSRVTT/TrainValVideo/video2960.mp4',\n",
       "  'y': 'a cartoon character runs around inside of a video game',\n",
       "  'x_embed': array([-0.01323111,  0.01547959, -0.01875113, ..., -0.02266031,\n",
       "          0.00901074,  0.02306143], dtype=float32),\n",
       "  'y_embed': array([ 0.00500106,  0.02801896, -0.02620777, ...,  0.01640674,\n",
       "         -0.00740285,  0.01365293], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/MSRVTT/TrainValVideo/video2960.mp4',\n",
       "  'y': 'a character is running in the snow',\n",
       "  'x_embed': array([-0.01323111,  0.01547959, -0.01875113, ..., -0.02266031,\n",
       "          0.00901074,  0.02306143], dtype=float32),\n",
       "  'y_embed': array([ 0.01420645, -0.02061816,  0.01971353, ...,  0.0154368 ,\n",
       "         -0.00989074,  0.04984858], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/MSRVTT/TrainValVideo/video2960.mp4',\n",
       "  'y': 'a person plays a video game centered around ice age the movie',\n",
       "  'x_embed': array([-0.01323111,  0.01547959, -0.01875113, ..., -0.02266031,\n",
       "          0.00901074,  0.02306143], dtype=float32),\n",
       "  'y_embed': array([ 0.00598882,  0.02546242, -0.04019066, ..., -0.01567093,\n",
       "          0.02927003,  0.01471767], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/MSRVTT/TrainValVideo/video2960.mp4',\n",
       "  'y': 'a person plays online and records themselves',\n",
       "  'x_embed': array([-0.01323111,  0.01547959, -0.01875113, ..., -0.02266031,\n",
       "          0.00901074,  0.02306143], dtype=float32),\n",
       "  'y_embed': array([ 0.04546095,  0.06368332, -0.02633356, ...,  0.02213986,\n",
       "          0.04960563, -0.00956051], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/MSRVTT/TrainValVideo/video2960.mp4',\n",
       "  'y': 'a scene from the ice age video game is shown',\n",
       "  'x_embed': array([-0.01323111,  0.01547959, -0.01875113, ..., -0.02266031,\n",
       "          0.00901074,  0.02306143], dtype=float32),\n",
       "  'y_embed': array([-0.01493739,  0.01311338, -0.01299921, ..., -0.03125907,\n",
       "          0.01141302,  0.02828832], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/MSRVTT/TrainValVideo/video2960.mp4',\n",
       "  'y': 'a video game character is jumping about in a cave',\n",
       "  'x_embed': array([-0.01323111,  0.01547959, -0.01875113, ..., -0.02266031,\n",
       "          0.00901074,  0.02306143], dtype=float32),\n",
       "  'y_embed': array([-0.0185261 , -0.00820488, -0.05367953, ...,  0.0289044 ,\n",
       "         -0.01674702,  0.00338483], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/MSRVTT/TrainValVideo/video2960.mp4',\n",
       "  'y': 'a video game of a little animal running through an ice tunnel',\n",
       "  'x_embed': array([-0.01323111,  0.01547959, -0.01875113, ..., -0.02266031,\n",
       "          0.00901074,  0.02306143], dtype=float32),\n",
       "  'y_embed': array([-0.01202287,  0.0498185 , -0.04836721, ...,  0.00138053,\n",
       "         -0.01445785,  0.03296154], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/MSRVTT/TrainValVideo/video2960.mp4',\n",
       "  'y': 'a video game of a small animal',\n",
       "  'x_embed': array([-0.01323111,  0.01547959, -0.01875113, ..., -0.02266031,\n",
       "          0.00901074,  0.02306143], dtype=float32),\n",
       "  'y_embed': array([ 0.0108626 ,  0.01268917, -0.01462863, ...,  0.00971311,\n",
       "          0.01844065,  0.01923237], dtype=float32),\n",
       "  'split': 'train'},\n",
       " {'x': '/pasteur/u/yuhuiz/data/MSRVTT/TrainValVideo/video2960.mp4',\n",
       "  'y': 'a video shows gameplay from ice age',\n",
       "  'x_embed': array([-0.01323111,  0.01547959, -0.01875113, ..., -0.02266031,\n",
       "          0.00901074,  0.02306143], dtype=float32),\n",
       "  'y_embed': array([-0.00295102,  0.00283961,  0.00259865, ..., -0.00960097,\n",
       "          0.00566897,  0.00613298], dtype=float32),\n",
       "  'split': 'train'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSCOCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sailhome/yuhuiz/develop/miniconda3/envs/modeldiff/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/sailhome/yuhuiz/develop/miniconda3/envs/modeldiff/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "import data\n",
    "import torch\n",
    "from models import imagebind_model\n",
    "from models.imagebind_model import ModalityType\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "COCO_PATH = \"/pasteur/u/yuhuiz/data/COCO/\"\n",
    "\n",
    "captions_train = json.load(open(COCO_PATH + \"annotations/captions_train2017.json\"))\n",
    "captions_val = json.load(open(COCO_PATH + \"annotations/captions_val2017.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLIP\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'license': 3, 'file_name': '000000391895.jpg', 'coco_url': 'http://images.cocodataset.org/train2017/000000391895.jpg', 'height': 360, 'width': 640, 'date_captured': '2013-11-14 11:18:45', 'flickr_url': 'http://farm9.staticflickr.com/8186/8119368305_4e622c8349_z.jpg', 'id': 391895}\n",
      "/pasteur/u/yuhuiz/data/COCO/images/train2017/000000391895.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3853/3853 [17:39<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616767 A bicycle replica with a clock as the front wheel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19274/19274 [05:08<00:00, 62.42it/s]\n"
     ]
    }
   ],
   "source": [
    "all_images = captions_train[\"images\"] + captions_val[\"images\"]\n",
    "print(all_images[0])\n",
    "all_image_filenames = [\"/pasteur/u/yuhuiz/data/COCO/images/\" + img[\"coco_url\"].replace(\"http://images.cocodataset.org/\", \"\") for img in all_images]\n",
    "print(all_image_filenames[0])\n",
    "\n",
    "image_embeddings = []\n",
    "\n",
    "batch_size = 32\n",
    "for i in trange(0, len(all_images), batch_size):\n",
    "    # with torch.no_grad():\n",
    "    #     # Load data\n",
    "    #     inputs = {\n",
    "    #         ModalityType.VISION: data.load_and_transform_vision_data(all_image_filenames[i: i + batch_size], device),\n",
    "    #     }\n",
    "    #     embeddings = model(inputs)\n",
    "    #     image_embeddings.append(embeddings[ModalityType.VISION].cpu())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image = torch.stack([preprocess(Image.open(im)) for im in all_image_filenames[i: i + batch_size]]).to(device)\n",
    "        image_features = model.encode_image(image)\n",
    "        image_embeddings.append(image_features.cpu())\n",
    "        # text_features = model.encode_text(text)\n",
    "\n",
    "image_embeddings = torch.cat(image_embeddings, dim=0)\n",
    "torch.save([all_image_filenames, image_embeddings], \"image_embeddings_coco_clip.pt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "annotations = captions_train[\"annotations\"] + captions_val[\"annotations\"]\n",
    "captions = [annotation[\"caption\"] for annotation in annotations]\n",
    "print(len(captions), captions[0])\n",
    "\n",
    "text_embeddings = []\n",
    "\n",
    "batch_size = 32\n",
    "for i in trange(0, len(captions), batch_size):\n",
    "    # with torch.no_grad():\n",
    "    #     # Load data\n",
    "    #     inputs = {\n",
    "    #         ModalityType.TEXT: data.load_and_transform_text(captions[i: i + batch_size], device),\n",
    "    #     }\n",
    "    #     embeddings = model(inputs)\n",
    "    #     text_embeddings.append(embeddings[ModalityType.TEXT].cpu())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text = clip.tokenize(captions[i: i + batch_size]).to(device)\n",
    "        text_features = model.encode_text(text)\n",
    "        text_embeddings.append(text_features.cpu())\n",
    "\n",
    "\n",
    "text_embeddings = torch.cat(text_embeddings, dim=0)\n",
    "torch.save([captions, text_embeddings], \"text_embeddings_coco_clip.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 6\n",
      "126 6\n",
      "619 6\n",
      "818 6\n",
      "1205 6\n",
      "1331 6\n",
      "1611 6\n",
      "1753 6\n",
      "2064 6\n",
      "2114 6\n",
      "2193 6\n",
      "2395 6\n",
      "2939 6\n",
      "3127 6\n",
      "3305 6\n",
      "3533 6\n",
      "3550 6\n",
      "3942 6\n",
      "4200 6\n",
      "4396 6\n",
      "4947 6\n",
      "5227 6\n",
      "7459 6\n",
      "7482 6\n",
      "8156 6\n",
      "8263 6\n",
      "8466 6\n",
      "9010 6\n",
      "9268 6\n",
      "9350 6\n",
      "9441 6\n",
      "10244 6\n",
      "10528 6\n",
      "10548 6\n",
      "10761 6\n",
      "10946 6\n",
      "11143 6\n",
      "11504 6\n",
      "11930 6\n",
      "12048 6\n",
      "12513 6\n",
      "12572 6\n",
      "12638 6\n",
      "13166 6\n",
      "13829 6\n",
      "13988 6\n",
      "14621 6\n",
      "15050 6\n",
      "15097 6\n",
      "15247 6\n",
      "15451 6\n",
      "15601 6\n",
      "15603 6\n",
      "15783 6\n",
      "16719 6\n",
      "16730 6\n",
      "17440 6\n",
      "17657 6\n",
      "18158 6\n",
      "18200 6\n",
      "18938 6\n",
      "19145 6\n",
      "19963 6\n",
      "21319 6\n",
      "21411 6\n",
      "21611 6\n",
      "21818 6\n",
      "21932 6\n",
      "22206 6\n",
      "22268 6\n",
      "22570 7\n",
      "23322 6\n",
      "23859 6\n",
      "24063 6\n",
      "24448 6\n",
      "24498 6\n",
      "24509 6\n",
      "24518 6\n",
      "25208 6\n",
      "26268 6\n",
      "26402 6\n",
      "26473 6\n",
      "26728 6\n",
      "26737 6\n",
      "26925 6\n",
      "27203 6\n",
      "27391 6\n",
      "27576 6\n",
      "27593 6\n",
      "27605 6\n",
      "27703 6\n",
      "27770 6\n",
      "28082 6\n",
      "28556 6\n",
      "28623 6\n",
      "28657 7\n",
      "28888 6\n",
      "29290 6\n",
      "29343 6\n",
      "29620 6\n",
      "29697 6\n",
      "30469 6\n",
      "30572 6\n",
      "30818 6\n",
      "31408 6\n",
      "31704 6\n",
      "31786 6\n",
      "31920 6\n",
      "32375 6\n",
      "32996 6\n",
      "33300 6\n",
      "33417 6\n",
      "33474 6\n",
      "33629 6\n",
      "33741 6\n",
      "34260 6\n",
      "35009 6\n",
      "35499 6\n",
      "35563 6\n",
      "35814 6\n",
      "36579 6\n",
      "36844 6\n",
      "37083 6\n",
      "37290 6\n",
      "37689 6\n",
      "37713 6\n",
      "38251 6\n",
      "38640 6\n",
      "38830 6\n",
      "39206 6\n",
      "39476 6\n",
      "39713 6\n",
      "39956 6\n",
      "40529 6\n",
      "40660 6\n",
      "40909 6\n",
      "41005 6\n",
      "41521 6\n",
      "42079 6\n",
      "42835 6\n",
      "43413 6\n",
      "44310 6\n",
      "44808 6\n",
      "45097 6\n",
      "45197 6\n",
      "45341 6\n",
      "46429 6\n",
      "46469 6\n",
      "46575 6\n",
      "46618 6\n",
      "47146 6\n",
      "47390 6\n",
      "48357 6\n",
      "48789 6\n",
      "49548 6\n",
      "49625 7\n",
      "50616 6\n",
      "50724 6\n",
      "51154 6\n",
      "51325 6\n",
      "51629 6\n",
      "52540 6\n",
      "53580 6\n",
      "53666 6\n",
      "53954 6\n",
      "54788 6\n",
      "55058 6\n",
      "55133 6\n",
      "56545 6\n",
      "57125 6\n",
      "57581 6\n",
      "57659 6\n",
      "58908 6\n",
      "59672 6\n",
      "60382 6\n",
      "60584 6\n",
      "61225 6\n",
      "61424 6\n",
      "61515 6\n",
      "61870 6\n",
      "63418 6\n",
      "63432 6\n",
      "65719 6\n",
      "65892 6\n",
      "65945 6\n",
      "66468 6\n",
      "66679 6\n",
      "67095 6\n",
      "67528 6\n",
      "67669 6\n",
      "67803 6\n",
      "67978 6\n",
      "67998 6\n",
      "68136 6\n",
      "69360 6\n",
      "69855 6\n",
      "70204 6\n",
      "72031 6\n",
      "72238 6\n",
      "72516 6\n",
      "73390 6\n",
      "73622 6\n",
      "74546 6\n",
      "75102 6\n",
      "75689 6\n",
      "75861 6\n",
      "75939 6\n",
      "77006 6\n",
      "77024 6\n",
      "77033 6\n",
      "77304 6\n",
      "77634 6\n",
      "77658 6\n",
      "77721 6\n",
      "77754 6\n",
      "77834 6\n",
      "78267 6\n",
      "78427 6\n",
      "78747 6\n",
      "79097 6\n",
      "79935 6\n",
      "80093 6\n",
      "80568 6\n",
      "81944 6\n",
      "82219 6\n",
      "82242 6\n",
      "83019 6\n",
      "84022 6\n",
      "84188 6\n",
      "84367 6\n",
      "85428 6\n",
      "85868 6\n",
      "86016 6\n",
      "86974 6\n",
      "87733 6\n",
      "88099 6\n",
      "88381 6\n",
      "88441 6\n",
      "88459 6\n",
      "89043 6\n",
      "89109 6\n",
      "90203 6\n",
      "90291 6\n",
      "90523 6\n",
      "90654 6\n",
      "90938 6\n",
      "91353 6\n",
      "91712 6\n",
      "92438 6\n",
      "92589 6\n",
      "93061 6\n",
      "94257 6\n",
      "94935 6\n",
      "96198 6\n",
      "97051 6\n",
      "97277 6\n",
      "97340 6\n",
      "97435 6\n",
      "98204 6\n",
      "98479 6\n",
      "99039 6\n",
      "99383 6\n",
      "99650 6\n",
      "99674 6\n",
      "99880 6\n",
      "99958 6\n",
      "100118 6\n",
      "100693 6\n",
      "100904 6\n",
      "101009 6\n",
      "101194 6\n",
      "101398 6\n",
      "101404 6\n",
      "102468 6\n",
      "102741 6\n",
      "103779 6\n",
      "104031 6\n",
      "104549 6\n",
      "105241 6\n",
      "105512 6\n",
      "105803 6\n",
      "106019 6\n",
      "106160 6\n",
      "106232 6\n",
      "106566 6\n",
      "107044 6\n",
      "107278 6\n",
      "107393 6\n",
      "107647 6\n",
      "107983 6\n",
      "108458 6\n",
      "108487 6\n",
      "108646 6\n",
      "108690 6\n",
      "108799 6\n",
      "109185 6\n",
      "110853 6\n",
      "111263 6\n",
      "111466 6\n",
      "111801 6\n",
      "111850 6\n",
      "112217 6\n",
      "112337 6\n",
      "114323 6\n",
      "114351 6\n",
      "114377 6\n",
      "114813 6\n",
      "114828 6\n",
      "115649 6\n",
      "115838 6\n",
      "116221 6\n",
      "117047 6\n",
      "117105 6\n",
      "117294 6\n",
      "117648 6\n",
      "102 6\n",
      "592 6\n",
      "602 6\n",
      "982 6\n",
      "987 6\n",
      "1565 7\n",
      "1723 6\n",
      "1726 6\n",
      "1773 6\n",
      "3661 6\n",
      "3736 6\n",
      "4561 6\n",
      "4669 6\n"
     ]
    }
   ],
   "source": [
    "imageid2image = {}\n",
    "\n",
    "for item in captions_train[\"images\"] + captions_val[\"images\"]:\n",
    "    imageid2image[item[\"id\"]] = item\n",
    "    item[\"captions\"] = []\n",
    "\n",
    "for ann in captions_train[\"annotations\"] + captions_val[\"annotations\"]:\n",
    "    image_id = ann['image_id']\n",
    "    caption = ann['caption']\n",
    "    image = imageid2image[image_id]\n",
    "    image[\"captions\"].append(caption)\n",
    "\n",
    "\n",
    "\n",
    "all_image_filenames, image_embeddings = torch.load(\"image_embeddings_coco_clip.pt\")\n",
    "captions, text_embeddings = torch.load(\"text_embeddings_coco_clip.pt\")\n",
    "image_embeddings = image_embeddings.float()\n",
    "text_embeddings = text_embeddings.float()\n",
    "\n",
    "image2embeddings = dict(zip(all_image_filenames, image_embeddings))\n",
    "text2embeddings = dict(zip(captions, text_embeddings))\n",
    "\n",
    "\n",
    "# transform to x_embed (audio embedding), y_embed (caption embedding), y (caption)\n",
    "train_data = []\n",
    "\n",
    "for idx, item in enumerate(captions_train[\"images\"]):\n",
    "    captions = item[\"captions\"]\n",
    "    file_name = \"/pasteur/u/yuhuiz/data/COCO/images/\" + item[\"coco_url\"].replace(\"http://images.cocodataset.org/\", \"\")\n",
    "    if len(captions) != 5:\n",
    "        print(idx, len(captions))\n",
    "\n",
    "    for i in range(5): # 5 captions\n",
    "        train_data.append({\n",
    "            \"x\": file_name,\n",
    "            \"y\": captions[i],\n",
    "            \"x_embed\": F.normalize(image2embeddings[file_name], dim=0).numpy(),\n",
    "            \"y_embed\": F.normalize(text2embeddings[captions[i]], dim=0).numpy(),\n",
    "            \"split\": \"train\"\n",
    "        })\n",
    "\n",
    "test_data = []\n",
    "\n",
    "for idx, item in enumerate(captions_val[\"images\"]):\n",
    "    captions = item[\"captions\"]\n",
    "    file_name = \"/pasteur/u/yuhuiz/data/COCO/images/\" + item[\"coco_url\"].replace(\"http://images.cocodataset.org/\", \"\")\n",
    "    if len(captions) != 5:\n",
    "        print(idx, len(captions))\n",
    "\n",
    "    for i in range(5): # 5 captions\n",
    "        test_data.append({\n",
    "            \"x\": file_name,\n",
    "            \"y\": captions[i],\n",
    "            \"x_embed\": F.normalize(image2embeddings[file_name], dim=0).numpy(),\n",
    "            \"y_embed\": F.normalize(text2embeddings[captions[i]], dim=0).numpy(),\n",
    "            \"split\": \"test\"\n",
    "        })\n",
    "\n",
    "\n",
    "data = train_data + test_data\n",
    "\n",
    "import pickle \n",
    "with open('data_image_coco_clip.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modeldiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
