{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import dnnlib, legacy\n",
    "import clip\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "from compute_embed_mean import TEXT_EMBED_MEAN_LAFITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, device, path):\n",
    "        self.name = 'generator'\n",
    "        self.model = self.load_model(device, path)\n",
    "        self.device = device\n",
    "        self.force_32 = False\n",
    "        \n",
    "    def load_model(self, device, path):\n",
    "        with dnnlib.util.open_url(path) as f:\n",
    "            network= legacy.load_network_pkl(f)\n",
    "            self.G_ema = network['G_ema'].to(device)\n",
    "            self.D = network['D'].to(device)\n",
    "#                 self.G = network['G'].to(device)\n",
    "            return self.G_ema\n",
    "        \n",
    "    def generate(self, z, c, fts, noise_mode='const', return_styles=True):\n",
    "        return self.model(z, c, fts=fts, noise_mode=noise_mode, return_styles=return_styles, force_fp32=self.force_32)\n",
    "    \n",
    "    def generate_from_style(self, style, noise_mode='const'):\n",
    "        ws = torch.randn(1, self.model.num_ws, 512)\n",
    "        return self.model.synthesis(ws, fts=None, styles=style, noise_mode=noise_mode, force_fp32=self.force_32)\n",
    "    \n",
    "    def tensor_to_img(self, tensor):\n",
    "        img = torch.clamp((tensor + 1.) * 127.5, 0., 255.)\n",
    "        img_list = img.permute(0, 2, 3, 1)\n",
    "        img_list = [img for img in img_list]\n",
    "        return Image.fromarray(torch.cat(img_list, dim=-2).detach().cpu().numpy().astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a1b5b-6f1a-42b8-b4b2-98613dc4446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_ckpt = ...\n",
    "c2_1_ckpt = ...\n",
    "c2_2_ckpt = ...\n",
    "c3_ckpt = ...\n",
    "lafite_ckpt = ...\n",
    "\n",
    "models = ['c1', 'c2_1', 'c2_2', 'c3', 'lafite']\n",
    "\n",
    "models_to_paths = dict(zip(models, [c1_ckpt, c2_1_ckpt, c2_2_ckpt, c3_ckpt, lafite_ckpt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a707f8f-e9e3-4955-b78d-6af284d9b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEXT_EMBED_MEAN_LAFITE, 'rb') as f:\n",
    "    text_mean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dfcfcc-898a-4cf4-89fd-5750e6862da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"A small town street with old buildings\",\n",
    "    \"Large bus transporting on a open road into a city.\",\n",
    "    \"A BIRD ON A BRANCH RUFFLING ITS FEATHERS\"\n",
    "]\n",
    "\n",
    "img_ids = [\n",
    "    30984,\n",
    "    26839,\n",
    "    14574\n",
    "]\n",
    "\n",
    "img_id_to_sentence = dict(zip(img_ids, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29873681",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' # please use GPU, do not use CPU\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model = clip_model.eval()\n",
    "\n",
    "text_mean = text_mean.to(device)\n",
    "\n",
    "os.makedirs(\"generated\", exist_ok=True)\n",
    "\n",
    "for model in models_to_paths:\n",
    "    print(f\"Model: {model}\")\n",
    "    with torch.no_grad():\n",
    "        path = models_to_paths[model]  # pre-trained model\n",
    "        generator = Generator(device=device, path=path)\n",
    "        \n",
    "        for img_id in img_id_to_sentence:\n",
    "            txt = img_id_to_sentence[img_id]\n",
    "            print(f\"Sentence: {txt}\")\n",
    "            tokenized_text = clip.tokenize([txt]).to(device)\n",
    "            txt_fts = clip_model.encode_text(tokenized_text)\n",
    "            txt_fts = txt_fts/txt_fts.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            if model in ['c2_1', 'c3']:\n",
    "                txt_fts = txt_fts - text_mean\n",
    "                txt_fts = txt_fts/txt_fts.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            z = torch.randn((1, 512)).to(device)\n",
    "            c = torch.randn((1, 1)).to(device) # label is actually not used\n",
    "            img, _ = generator.generate(z=z, c=c, fts=txt_fts)\n",
    "            to_show_img = generator.tensor_to_img(img)\n",
    "            to_show_img.save(f'generated/{model}_img_{img_id}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf3e35-ca72-4dd3-bd3b-a65bf7ccae87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
