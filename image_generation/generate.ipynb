{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "import dnnlib, legacy\n",
    "import clip\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Generator:\n",
    "    def __init__(self, device, path):\n",
    "        self.name = 'generator'\n",
    "        self.model = self.load_model(device, path)\n",
    "        self.device = device\n",
    "        self.force_32 = False\n",
    "        \n",
    "    def load_model(self, device, path):\n",
    "        with dnnlib.util.open_url(path) as f:\n",
    "            network= legacy.load_network_pkl(f)\n",
    "            self.G_ema = network['G_ema'].to(device)\n",
    "            self.D = network['D'].to(device)\n",
    "#                 self.G = network['G'].to(device)\n",
    "            return self.G_ema\n",
    "        \n",
    "    def generate(self, z, c, fts, noise_mode='const', return_styles=True):\n",
    "        return self.model(z, c, fts=fts, noise_mode=noise_mode, return_styles=return_styles, force_fp32=self.force_32)\n",
    "    \n",
    "    def generate_from_style(self, style, noise_mode='const'):\n",
    "        ws = torch.randn(1, self.model.num_ws, 512)\n",
    "        return self.model.synthesis(ws, fts=None, styles=style, noise_mode=noise_mode, force_fp32=self.force_32)\n",
    "    \n",
    "    def tensor_to_img(self, tensor):\n",
    "        img = torch.clamp((tensor + 1.) * 127.5, 0., 255.)\n",
    "        img_list = img.permute(0, 2, 3, 1)\n",
    "        img_list = [img for img in img_list]\n",
    "        return Image.fromarray(torch.cat(img_list, dim=-2).detach().cpu().numpy().astype(np.uint8))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = 'cuda:0'\n",
    "\n",
    "text_mean_path = ...\n",
    "\n",
    "with open(text_mean_path, 'rb') as f:\n",
    "    txt_mean = pickle.load(f).to(device)\n",
    "\n",
    "def get_ground_truth_image(img_path):\n",
    "    if os.path.exists(img_path):\n",
    "        img = Image.open(img_path)\n",
    "        img = np.array(img)\n",
    "    else:\n",
    "        img = np.ones((224, 224, 3))\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def run_generation(model_paths, img_path, sentence):\n",
    "    with torch.no_grad():\n",
    "        clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "        clip_model = clip_model.eval()\n",
    "\n",
    "        tokenized_text = clip.tokenize([sentence]).to(device)\n",
    "        txt_fts = clip_model.encode_text(tokenized_text)\n",
    "        txt_fts = txt_fts/txt_fts.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        z = torch.randn((1, 512)).to(device)\n",
    "        c = torch.randn((1, 1)).to(device) # label is actually not used\n",
    "\n",
    "        images = {}\n",
    "\n",
    "        print(f\"Image path: {img_path}\")\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        for model_type, path in model_paths.items():\n",
    "            if model_type == 'ground_truth':\n",
    "                continue\n",
    "            elif model_type in ['c21', 'c3']:\n",
    "                txt_fts -= txt_mean\n",
    "                txt_fts = txt_fts/txt_fts.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            generator = Generator(device=device, path=path)\n",
    "\n",
    "            img, _ = generator.generate(z=z, c=c, fts=txt_fts)\n",
    "            to_show_img = generator.tensor_to_img(img)\n",
    "            images[model_type] = to_show_img\n",
    "        \n",
    "        images['ground_truth'] = get_ground_truth_image(img_path)\n",
    "\n",
    "        plt.figure(figsize=(10 * len(model_paths), 40 * len(model_paths))) \n",
    "\n",
    "        for i, model_type in enumerate(model_paths):\n",
    "            plt.subplot(1, len(model_paths), i+1) \n",
    "            plt.axis('off')\n",
    "            # plt.title(model_type)\n",
    "            plt.imshow(images[model_type])\n",
    "\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_paths = {\n",
    "    'ground_truth': None,\n",
    "    'lafite_reprod': ...,\n",
    "    'c1': ...,\n",
    "    'c21': ...,\n",
    "    'c22': ...,\n",
    "    'c3': ...,\n",
    "}\n",
    "\n",
    "\n",
    "sentences = {\n",
    "    \"path/to/image\": \"caption\"\n",
    "}\n",
    "\n",
    "for img_path, sentence in sentences.items():\n",
    "    torch.manual_seed(1234)\n",
    "    run_generation(model_paths, img_path, sentence)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}